{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90024191",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ffbb2ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN model refer to https://arxiv.org/abs/1609.00408\n",
    "class FaceCNN(nn.Module):\n",
    "    def __init__(self, num_classes=40):\n",
    "        super(FaceCNN, self).__init__()\n",
    "        \n",
    "        # Using the sequential model architecture as provided\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, stride=1, padding=1),  # (1)\n",
    "            nn.LeakyReLU(0.01),                                                             # (2)\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),                                          # (3)\n",
    "            \n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1), # (4)\n",
    "            nn.LeakyReLU(0.01),                                                             # (5)\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),                                          # (6)\n",
    "            \n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1),# (7)\n",
    "            nn.LeakyReLU(0.01),                                                             # (8)\n",
    "            nn.MaxPool2d(kernel_size=3, stride=3),                                          # (9)\n",
    "            nn.Flatten(),                                                                   # (10)\n",
    "            nn.Linear(in_features=128 * 7 * 9, out_features=1024),                                 # (11)\n",
    "            nn.LeakyReLU(0.01),                                                             # (12)\n",
    "            nn.Dropout(p=0.5),                                                              # (13)\n",
    "            nn.Linear(in_features=1024, out_features=40),                                   # (14)\n",
    "            nn.LogSoftmax(dim=1)                                                            # (15)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c6ed583f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Dataset for loading PGM files\n",
    "class PGMFaceDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.samples = []\n",
    "        \n",
    "        # Parse directory structure\n",
    "        for class_idx in range(1, 41):  # s1 to s40\n",
    "            class_dir = os.path.join(root_dir, f\"s{class_idx}\")\n",
    "            if os.path.isdir(class_dir):\n",
    "                for file_name in os.listdir(class_dir):\n",
    "                    if file_name.endswith('.pgm'):\n",
    "                        self.samples.append((\n",
    "                            os.path.join(class_dir, file_name),\n",
    "                            class_idx - 1  # 0-based index for classes\n",
    "                        ))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label = self.samples[idx]\n",
    "        \n",
    "        # Open PGM file with PIL\n",
    "        image = Image.open(img_path)\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef1fd0f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function with top-1 and top-5 metrics and early stopping\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, \n",
    "                num_epochs=10, patience=5, device='cuda'):\n",
    "    best_acc = 0.0\n",
    "    best_epoch = -1\n",
    "    patience_counter = 0\n",
    "    \n",
    "    history = {\n",
    "        'train_loss': [], 'train_top1_acc': [],\n",
    "        'val_loss': [], 'val_top1_acc': [], 'val_top5_acc': []\n",
    "    }\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        top1_correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Backward pass and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Statistics\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            top1_correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        # Calculate training metrics\n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "        epoch_top1_acc = top1_correct / total\n",
    "        \n",
    "        # Save training metrics\n",
    "        history['train_loss'].append(epoch_loss)\n",
    "        history['train_top1_acc'].append(epoch_top1_acc)\n",
    "        \n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {epoch_loss:.4f}, Train Top-1 Acc: {epoch_top1_acc:.4f}')\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_top1_correct = 0\n",
    "        val_top5_correct = 0\n",
    "        val_total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                \n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                val_loss += loss.item() * inputs.size(0)\n",
    "                \n",
    "                # For top-1 accuracy\n",
    "                _, top1_preds = torch.max(outputs, 1)\n",
    "                val_top1_correct += (top1_preds == labels).sum().item()\n",
    "                \n",
    "                # For top-5 accuracy (or fewer if num_classes < 5)\n",
    "                k = min(5, outputs.size(1))\n",
    "                _, top5_preds = torch.topk(outputs, k, dim=1)\n",
    "                for i, label in enumerate(labels):\n",
    "                    if label in top5_preds[i]:\n",
    "                        val_top5_correct += 1\n",
    "                \n",
    "                val_total += labels.size(0)\n",
    "        \n",
    "        # Calculate validation metrics\n",
    "        val_epoch_loss = val_loss / len(val_loader.dataset)\n",
    "        val_top1_acc = val_top1_correct / val_total\n",
    "        val_top5_acc = val_top5_correct / val_total\n",
    "        \n",
    "        # Save validation metrics\n",
    "        history['val_loss'].append(val_epoch_loss)\n",
    "        history['val_top1_acc'].append(val_top1_acc)\n",
    "        history['val_top5_acc'].append(val_top5_acc)\n",
    "        \n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Val Loss: {val_epoch_loss:.4f}, '\n",
    "              f'Val Top-1 Acc: {val_top1_acc:.4f}, Val Top-5 Acc: {val_top5_acc:.4f}')\n",
    "        \n",
    "        # Check if current model is the best\n",
    "        if val_top1_acc > best_acc:\n",
    "            best_acc = val_top1_acc\n",
    "            best_epoch = epoch\n",
    "            patience_counter = 0  # Reset patience counter\n",
    "            \n",
    "            # Save best model\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'val_top1_acc': val_top1_acc,\n",
    "                'val_top5_acc': val_top5_acc,\n",
    "                'train_loss': epoch_loss,\n",
    "                'val_loss': val_epoch_loss\n",
    "            }, 'best_face_cnn.pth')\n",
    "            \n",
    "            print(f'New best model saved with Val Top-1 Acc: {val_top1_acc:.4f}')\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            print(f'Patience counter: {patience_counter}/{patience}')\n",
    "        \n",
    "        # Check for early stopping\n",
    "        if patience_counter >= patience:\n",
    "            print(f'Early stopping triggered after epoch {epoch+1}')\n",
    "            print(f'Best model was at epoch {best_epoch+1} with Val Top-1 Acc: {best_acc:.4f}')\n",
    "            break\n",
    "    \n",
    "    \n",
    "    print(f'Training completed after {epoch+1} epochs')\n",
    "    if best_epoch != epoch:\n",
    "        print(f'Best model was at epoch {best_epoch+1} with Val Top-1 Acc: {best_acc:.4f}')\n",
    "    \n",
    "    # Return the trained model and training history\n",
    "    return model, history, best_epoch+1\n",
    "\n",
    "# Evaluation function with top-1 and top-5 accuracy\n",
    "def evaluate_model(model, test_loader, device='cuda'):\n",
    "    model.eval()\n",
    "    top1_correct = 0\n",
    "    top5_correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            # For top-1 accuracy\n",
    "            _, top1_preds = torch.max(outputs, 1)\n",
    "            top1_correct += (top1_preds == labels).sum().item()\n",
    "            \n",
    "            # For top-5 accuracy (or fewer if num_classes < 5)\n",
    "            k = min(5, outputs.size(1))\n",
    "            _, top5_preds = torch.topk(outputs, k, dim=1)\n",
    "            for i, label in enumerate(labels):\n",
    "                if label in top5_preds[i]:\n",
    "                    top5_correct += 1\n",
    "            \n",
    "            total += labels.size(0)\n",
    "    \n",
    "    top1_acc = top1_correct / total\n",
    "    top5_acc = top5_correct / total\n",
    "    \n",
    "    print(f'Test Top-1 Accuracy: {top1_acc:.4f} ({top1_correct}/{total})')\n",
    "    print(f'Test Top-5 Accuracy: {top5_acc:.4f} ({top5_correct}/{total})')\n",
    "    \n",
    "    return top1_acc, top5_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "66dc7a53",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Epoch 1/20, Train Loss: 3.7460, Train Top-1 Acc: 0.0179\n",
      "Epoch 1/20, Val Loss: 3.6725, Val Top-1 Acc: 0.0333, Val Top-5 Acc: 0.2000\n",
      "New best model saved with Val Top-1 Acc: 0.0333\n",
      "Epoch 2/20, Train Loss: 3.6588, Train Top-1 Acc: 0.0429\n",
      "Epoch 2/20, Val Loss: 3.5135, Val Top-1 Acc: 0.1500, Val Top-5 Acc: 0.4833\n",
      "New best model saved with Val Top-1 Acc: 0.1500\n",
      "Epoch 3/20, Train Loss: 3.1629, Train Top-1 Acc: 0.2107\n",
      "Epoch 3/20, Val Loss: 2.2849, Val Top-1 Acc: 0.4000, Val Top-5 Acc: 0.7333\n",
      "New best model saved with Val Top-1 Acc: 0.4000\n",
      "Epoch 4/20, Train Loss: 2.2192, Train Top-1 Acc: 0.3679\n",
      "Epoch 4/20, Val Loss: 1.5426, Val Top-1 Acc: 0.6833, Val Top-5 Acc: 0.9333\n",
      "New best model saved with Val Top-1 Acc: 0.6833\n",
      "Epoch 5/20, Train Loss: 1.5142, Train Top-1 Acc: 0.5643\n",
      "Epoch 5/20, Val Loss: 1.1281, Val Top-1 Acc: 0.6833, Val Top-5 Acc: 0.9000\n",
      "Patience counter: 1/7\n",
      "Epoch 6/20, Train Loss: 0.9851, Train Top-1 Acc: 0.7036\n",
      "Epoch 6/20, Val Loss: 0.6621, Val Top-1 Acc: 0.8167, Val Top-5 Acc: 1.0000\n",
      "New best model saved with Val Top-1 Acc: 0.8167\n",
      "Epoch 7/20, Train Loss: 0.6104, Train Top-1 Acc: 0.8357\n",
      "Epoch 7/20, Val Loss: 0.3714, Val Top-1 Acc: 0.8833, Val Top-5 Acc: 1.0000\n",
      "New best model saved with Val Top-1 Acc: 0.8833\n",
      "Epoch 8/20, Train Loss: 0.3280, Train Top-1 Acc: 0.8893\n",
      "Epoch 8/20, Val Loss: 0.3371, Val Top-1 Acc: 0.8667, Val Top-5 Acc: 1.0000\n",
      "Patience counter: 1/7\n",
      "Epoch 9/20, Train Loss: 0.2896, Train Top-1 Acc: 0.9179\n",
      "Epoch 9/20, Val Loss: 0.2512, Val Top-1 Acc: 0.9500, Val Top-5 Acc: 1.0000\n",
      "New best model saved with Val Top-1 Acc: 0.9500\n",
      "Epoch 10/20, Train Loss: 0.1739, Train Top-1 Acc: 0.9500\n",
      "Epoch 10/20, Val Loss: 0.2235, Val Top-1 Acc: 0.9500, Val Top-5 Acc: 1.0000\n",
      "Patience counter: 1/7\n",
      "Epoch 11/20, Train Loss: 0.1314, Train Top-1 Acc: 0.9536\n",
      "Epoch 11/20, Val Loss: 0.3286, Val Top-1 Acc: 0.9000, Val Top-5 Acc: 1.0000\n",
      "Patience counter: 2/7\n",
      "Epoch 12/20, Train Loss: 0.1088, Train Top-1 Acc: 0.9679\n",
      "Epoch 12/20, Val Loss: 0.1824, Val Top-1 Acc: 0.9500, Val Top-5 Acc: 1.0000\n",
      "Patience counter: 3/7\n",
      "Epoch 13/20, Train Loss: 0.1845, Train Top-1 Acc: 0.9464\n",
      "Epoch 13/20, Val Loss: 0.2525, Val Top-1 Acc: 0.9000, Val Top-5 Acc: 1.0000\n",
      "Patience counter: 4/7\n",
      "Epoch 14/20, Train Loss: 0.1381, Train Top-1 Acc: 0.9607\n",
      "Epoch 14/20, Val Loss: 0.2332, Val Top-1 Acc: 0.8833, Val Top-5 Acc: 1.0000\n",
      "Patience counter: 5/7\n",
      "Epoch 15/20, Train Loss: 0.0858, Train Top-1 Acc: 0.9679\n",
      "Epoch 15/20, Val Loss: 0.3051, Val Top-1 Acc: 0.8667, Val Top-5 Acc: 1.0000\n",
      "Patience counter: 6/7\n",
      "Epoch 16/20, Train Loss: 0.0490, Train Top-1 Acc: 0.9857\n",
      "Epoch 16/20, Val Loss: 0.1272, Val Top-1 Acc: 0.9500, Val Top-5 Acc: 1.0000\n",
      "Patience counter: 7/7\n",
      "Early stopping triggered after epoch 16\n",
      "Best model was at epoch 9 with Val Top-1 Acc: 0.9500\n",
      "Training completed after 16 epochs\n",
      "Best model was at epoch 9 with Val Top-1 Acc: 0.9500\n",
      "\n",
      "=== Final Model Evaluation ===\n",
      "Test Top-1 Accuracy: 0.8667 (52/60)\n",
      "Test Top-5 Accuracy: 0.9833 (59/60)\n",
      "\n",
      "=== Best Model Evaluation ===\n",
      "Test Top-1 Accuracy: 0.8333 (50/60)\n",
      "Test Top-5 Accuracy: 1.0000 (60/60)\n"
     ]
    }
   ],
   "source": [
    "# Main function\n",
    "def main():\n",
    "    # Set device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Set hyperparameters\n",
    "    batch_size = 32\n",
    "    num_epochs = 20\n",
    "    learning_rate = 0.001\n",
    "    patience = 7\n",
    "    \n",
    "    # Define transformations\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Grayscale(num_output_channels=1),  # Ensure grayscale\n",
    "        #transforms.Resize((112, 112)),  # Adjust based on your dataset to make in_features=8064 work\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.5], std=[0.5])  # Normalize grayscale images\n",
    "    ])\n",
    "    \n",
    "    # Create dataset\n",
    "    #os.path.join(\"..\", \"dp_pixelized_faces\", f\"block_size_2\", \"epsilon_0.1\")\n",
    "    #root_dir = os.path.join(\"..\", 'gaussian_faces', \"kernel_size_21\")  # Update this to your dataset path\n",
    "    root_dir = os.path.join(\"..\", \"dp_pixelized_faces\", f\"block_size_4\", \"epsilon_1\")\n",
    "    dataset = PGMFaceDataset(root_dir=root_dir, transform=transform)\n",
    "    \n",
    "    # Split dataset\n",
    "    train_idx, temp_idx = train_test_split(list(range(len(dataset))), test_size=0.3, stratify=[dataset.samples[i][1] for i in range(len(dataset))])\n",
    "    val_idx, test_idx = train_test_split(temp_idx, test_size=0.5, stratify=[dataset.samples[i][1] for i in temp_idx])\n",
    "    \n",
    "    train_dataset = torch.utils.data.Subset(dataset, train_idx)\n",
    "    val_dataset = torch.utils.data.Subset(dataset, val_idx)\n",
    "    test_dataset = torch.utils.data.Subset(dataset, test_idx)\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "    \n",
    "    # Create model, loss function, and optimizer\n",
    "    model = FaceCNN(num_classes=40).to(device)\n",
    "    criterion = nn.NLLLoss()  # Since we're using LogSoftmax as the final layer\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # Train the model with early stopping\n",
    "    model, training_history, best_epoch = train_model(\n",
    "        model, \n",
    "        train_loader, \n",
    "        val_loader, \n",
    "        criterion, \n",
    "        optimizer, \n",
    "        num_epochs=num_epochs,  # Set a high maximum, early stopping will likely trigger before this\n",
    "        patience=patience,     # Stop after 7 epochs without improvement\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    # Evaluate the model with enhanced metrics\n",
    "    print(\"\\n=== Final Model Evaluation ===\")\n",
    "    top1_acc, top5_acc = evaluate_model(model, test_loader, device)\n",
    "\n",
    "    # evaluate the best saved model\n",
    "    best_model = FaceCNN(num_classes=40).to(device)\n",
    "    checkpoint = torch.load('best_face_cnn.pth')\n",
    "    best_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    print(\"\\n=== Best Model Evaluation ===\")\n",
    "    best_top1_acc, best_top5_acc = evaluate_model(best_model, test_loader, device)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e55f4225",
   "metadata": {},
   "source": [
    "## Test the trained model performance on original face dataset (without blur nor pixelized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf7f805",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to test the model on a single image\n",
    "def test_single_image(model, image_path, transform, class_names=None, device='cuda'):\n",
    "    # Set model to evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    try:\n",
    "        # Load the image\n",
    "        image = Image.open(image_path)\n",
    "        print(f\"Successfully loaded image: {image_path}\")\n",
    "        print(f\"Original image size: {image.size}\")\n",
    "        \n",
    "        # Apply transformations\n",
    "        if transform:\n",
    "            image_tensor = transform(image)\n",
    "            print(f\"Transformed tensor shape: {image_tensor.shape}\")\n",
    "        \n",
    "        # Add batch dimension\n",
    "        image_tensor = image_tensor.unsqueeze(0).to(device)\n",
    "        \n",
    "        # Get prediction\n",
    "        with torch.no_grad():\n",
    "            output = model(image_tensor)\n",
    "            probabilities = torch.exp(output)\n",
    "            \n",
    "            # Get top 3 predictions\n",
    "            topk_probs, topk_indices = torch.topk(probabilities, 3)\n",
    "            \n",
    "            # Convert to numpy for easier handling\n",
    "            topk_probs = topk_probs.cpu().numpy()[0]\n",
    "            topk_indices = topk_indices.cpu().numpy()[0]\n",
    "            \n",
    "            # Print results\n",
    "            print(\"\\nTop 3 predictions:\")\n",
    "            for i in range(3):\n",
    "                class_idx = topk_indices[i]\n",
    "                prob = topk_probs[i]\n",
    "                class_name = f\"Class {class_idx}\" if class_names is None else class_names[class_idx]\n",
    "                print(f\"{i+1}. {class_name} - Probability: {prob:.4f} ({prob*100:.2f}%)\")\n",
    "            \n",
    "            # Return the top prediction\n",
    "            top_class_idx = topk_indices[0]\n",
    "            top_prob = topk_probs[0]\n",
    "            \n",
    "            return top_class_idx, top_prob\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error testing image {image_path}: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None, None\n",
    "# Function to test a pre-trained model on a single image (can be called from outside)\n",
    "def test_pretrained_model(model_path, image_path):\n",
    "    # Set device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Create transformation\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Grayscale(num_output_channels=1),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "    ])\n",
    "    \n",
    "    # Create class names dictionary\n",
    "    class_names = {}\n",
    "    for i in range(40):\n",
    "        class_names[i] = f\"Subject {i+1}\"\n",
    "    \n",
    "    # Load the model\n",
    "    model = FaceCNN(num_classes=40).to(device)\n",
    "    try:\n",
    "        checkpoint = torch.load('best_face_cnn.pth')\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        #model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "        print(f\"Successfully loaded model from {model_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model: {e}\")\n",
    "        return\n",
    "    \n",
    "    # Test the image\n",
    "    class_idx, prob = test_single_image(model, image_path, transform, class_names, device)\n",
    "    \n",
    "    if class_idx is not None:\n",
    "        print(f\"\\nFinal prediction: {class_names[class_idx]} with probability {prob:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7991e3e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pretrained_model(\"best_face_cnn.pth\", os.path.join(\"..\", \"att_faces\", \"s1\", \"2.pgm\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce616685",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
